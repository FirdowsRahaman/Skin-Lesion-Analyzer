{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbQPVrsgrPdv"
   },
   "source": [
    "# **Skin Lesion Analyzer with Deep Learning**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YqwfEeE1av1"
   },
   "source": [
    "### **Outline**\n",
    "Use these links to jump to specific sections of this project.\n",
    "\n",
    "1. Import Packages\n",
    "2. Load and Transform the Dataset\n",
    "3. Model Development\n",
    "4. Model Training \n",
    "5. Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaH4iOVSra0s"
   },
   "source": [
    "### 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbuL5F7rrRs9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zamPLMdj2zJR"
   },
   "source": [
    "### 2. Load and Transform the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d53dRWzC3AtS"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "class DatasetBuilder():\n",
    "    def __init__(self, base_dir, csv_file):\n",
    "        self.base_dir = base_dir\n",
    "        self.csv_file = csv_file\n",
    "        \n",
    "    def transform_df(self, base_dir, csv_file):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        image_path_dict = {os.path.splitext(os.path.basename(x))[0]: x \n",
    "                       for x in glob.glob(os.path.join(base_dir, '*', '*.jpg'))}\n",
    "        label_dict = {'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3,\n",
    "                      'mel': 4, 'nv': 5, 'vasc': 6}\n",
    "        df['image_path'] = df['image_id'].map(image_path_dict.get)\n",
    "        df['label_id'] = df['dx'].map(label_dict.get)\n",
    "        return df\n",
    "    \n",
    "    def split_df(self):\n",
    "        dataframe = self.transform_df(self.base_dir, self.csv_file)\n",
    "        train_df, val_df = train_test_split(dataframe, test_size=0.15)\n",
    "        train_df, test_df = train_test_split(train_df, test_size=0.10)\n",
    "        return train_df, val_df, test_df\n",
    "    \n",
    "    def get_labels(self, dataframe):\n",
    "        label_list = dataframe.label_id.values\n",
    "        labels = to_categorical(label_list, num_classes=7)\n",
    "        return labels\n",
    "    \n",
    "    def decode_image(self, filename, label=None, image_size=(224, 224)):\n",
    "        bits = tf.io.read_file(filename)\n",
    "        image = tf.image.decode_jpeg(bits, channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        image = tf.image.resize(image, image_size)\n",
    "        return image, label\n",
    "    \n",
    "    def input_fn(self, dataframe, batch_size=32, mode=None):\n",
    "        image_list = dataframe.image_path.values\n",
    "        labels = self.get_labels(dataframe)\n",
    "        ds = (tf.data.Dataset     \n",
    "                .from_tensor_slices((image_list, labels))\n",
    "                .map(self.decode_image, num_parallel_calls=AUTOTUNE)\n",
    "                .cache()\n",
    "                .repeat()\n",
    "                .shuffle(buffer_size = 10 * batch_size)\n",
    "                .batch(batch_size)\n",
    "                .prefetch(AUTOTUNE))\n",
    "        return ds\n",
    "\n",
    "    def create_dataset(self):\n",
    "        train_df, val_df, test_df = self.split_df()\n",
    "        train_ds = self.input_fn(train_df)\n",
    "        val_ds = self.input_fn(val_df)\n",
    "        return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVIpA6tJ3Tgs"
   },
   "source": [
    "### 3. Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvDFXQ_q3jZn"
   },
   "outputs": [],
   "source": [
    "image_shape = (224, 224, 3)\n",
    "base_learning_rate = 0.0001\n",
    "\n",
    "base_model = DenseNet121(input_shape=image_shape, \n",
    "                         include_top=False,\n",
    "                         weights='imagenet')\n",
    "base_model.trainable = True\n",
    "\n",
    "image_input = keras.Input(shape=image_shape)\n",
    "x = base_model(image_input)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(7, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs=image_input, outputs=x)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=base_learning_rate/10),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBB-6bGk4yao"
   },
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSQPxrE84l4s"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(params):\n",
    "    csv_file = params['csv_file']\n",
    "    base_dir = params['base_dir']\n",
    "    batch_size = params['batch_size']\n",
    "    num_epochs = params['num_epochs']\n",
    "    train_steps = params['train_steps']\n",
    "    val_steps = params['val_steps']\n",
    "\n",
    "    builder = DatasetBuilder(base_dir, csv_file)\n",
    "    train_ds, val_ds = builder.create_dataset()\n",
    "    \n",
    "    history = model.fit(train_ds,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_data=val_ds,\n",
    "                        steps_per_epoch=train_steps,\n",
    "                        validation_steps=val_steps)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzlUVJG75kTU"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'csv_file': '../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv',\n",
    "    'base_dir': '../input/skin-cancer-mnist-ham10000',\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 10,\n",
    "    'train_steps': 239,\n",
    "    'val_steps': 46\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVrOfLuU5nzF"
   },
   "outputs": [],
   "source": [
    "train_and_evaluate(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uf2tId25tbZ"
   },
   "source": [
    "### 5. Prediction and Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74GEGH5d3ctG"
   },
   "outputs": [],
   "source": [
    "def preprocess_input(params):\n",
    "    csv_file = params['csv_file']\n",
    "    base_dir = params['base_dir']\n",
    "    \n",
    "    builder = DatasetBuilder(base_dir, csv_file)\n",
    "    _, _, test_df = builder.split_df()\n",
    "    image_list = test_df.image_path.values\n",
    "    \n",
    "    image = []\n",
    "    for img in image_list:\n",
    "        img2array, _ = builder.decode_image(img)\n",
    "        img_batch = np.expand_dims(img2array, axis=0)\n",
    "        image.append(img_batch)\n",
    "    \n",
    "    labels = builder.get_labels(test_df)\n",
    "    images = np.vstack(image)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0IAHXXe33uQ"
   },
   "outputs": [],
   "source": [
    "test_images, test_labels = preprocess_input(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnKVKHZX38kE"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkP2Dob746nc"
   },
   "outputs": [],
   "source": [
    "true_labels = pd.DataFrame(data = test_labels) \n",
    "predicted_labels = pd.DataFrame(data = predictions) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Skin Lesion Analyzer Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
